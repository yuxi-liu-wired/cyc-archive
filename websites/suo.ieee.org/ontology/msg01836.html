<!-- MHonArc v2.4.8 -->
<!--X-Subject: SUO: Re: Propositions -->
<!--X-From-R13: Xba Ojoerl <wnjoerlNbnxynaq.rqh> -->
<!--X-Date: Sat, 24 Mar 2001 16:41:11 &#45;0500 (EST) -->
<!--X-Message-Id: 3ABD0E91.BD27294B@oakland.edu -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 3abc2da7.f017.0@bestweb.net -->
<!--X-Head-End-->

<!-- /groups/802/3/efm/public/email/msg01836.html -->
<!-- /groups/???? ?SUO?                              -->

<HTML>

<HEAD>
<TITLE>SUO: Re: Propositions</TITLE>
<LINK REV="made" HREF="mailto:jawbrey@oakland.edu">
</HEAD>

<BODY BGCOLOR="#FFFFFF">

<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->

<CENTER>

<TABLE CELLPADDING=3 CELLSPACING=0 BORDER=1 WIDTH="100%">
<TBODY>
<TR ALIGN="CENTER">
<TH COLSPAN=3><STRONG>Thread Links</STRONG></TH>
<TH COLSPAN=3><STRONG>Date Links</STRONG></TH>
</TR>
<TR ALIGN="CENTER">
<TD><A HREF="msg01825.html">Thread Prev</A>
</TD>
<TD><A HREF="msg01850.html">Thread Next</A>
</TD>
<TD><A HREF="thrd53.html#01836">Thread Index</A></Td>
<TD><A HREF="msg01837.html">Date Prev</A></TD>
<TD><A HREF="msg01835.html">Date Next</A>
</TD>
<TD><A HREF="mail54.html#01836">Date Index</A></TD>
</TR>
</TBODY>
</TABLE>
</CENTER>


<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<h1>SUO: Re: Propositions</h1>
<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<UL>
<LI><em>To</em>: &quot;John F. Sowa&quot; &lt;<A HREF="mailto:sowa@bestweb.net">sowa@bestweb.net</A>&gt;</LI>
<LI><em>Subject</em>: SUO: Re: Propositions</LI>
<LI><em>From</em>: Jon Awbrey &lt;<A HREF="mailto:jawbrey@oakland.edu">jawbrey@oakland.edu</A>&gt;</LI>
<LI><em>Date</em>: Sat, 24 Mar 2001 16:16:01 -0500</LI>
<LI><em>CC</em>: <A HREF="mailto:standard-upper-ontology@ieee.org">standard-upper-ontology@ieee.org</A></LI>
<LI><em>References</em>: &lt;<A HREF="msg01822.html">3abc2da7.f017.0@bestweb.net</A>&gt;</LI>
<LI><em>Reply-To</em>: Jon Awbrey &lt;<A HREF="mailto:jawbrey@oakland.edu">jawbrey@oakland.edu</A>&gt;</LI>
<LI><em>Sender</em>: <A HREF="mailto:owner-standard-upper-ontology@ieee.org">owner-standard-upper-ontology@ieee.org</A></LI>
</UL>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<hr>
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<PRE>

John F. Sowa wrote:
&gt; 
&gt; Pat and Jon,
&gt; 
&gt; JS&gt;&gt;&gt; I used the word "statement" to avoid getting into details
&gt; &gt;&gt; &gt; about the distinction between a "statement" and a "proposition".
&gt; &gt;&gt; &gt; But as I have said in my KR book (Ch. 5) and many email notes,
&gt; &gt;&gt; &gt; I prefer to define a proposition as an equivalence class of
&gt; &gt;&gt; &gt; statements in multiple languages.  With such a definition,
&gt; &gt;&gt; &gt; you can talk about entailment in a form that factors out as
&gt; &gt;&gt; &gt; much of the notation as you like.  (The larger the equivalence
&gt; &gt;&gt; &gt; class, the smaller the dependence on any particular notation.)
&gt; 
&gt; JS&gt;&gt;&gt; This kind of factoring is especially important for the SUO,
&gt; &gt;&gt; &gt; since we must deal with multiple languages, including CGs,
&gt; &gt;&gt; &gt; DAML, OIL, SQL, RDF, UML, and versions of controlled natural
&gt; &gt;&gt; &gt; languages, such as ACE or Aristotelian syllogisms.
&gt; 
&gt; PH&gt;&gt; Interesting idea, and I agree important.  But to give it flesh
&gt; &gt;&gt; you need to say what the equivalence relation is, relative to
&gt; &gt;&gt; which the equivalence classes are defined;  and that in turn
&gt; &gt;&gt; seems to require some way to compare the semantic content
&gt; &gt;&gt; of expressions from different languages.  If they are
&gt; &gt;&gt; all interpreted in the same structures one could see
&gt; &gt;&gt; how this would go -- equivalent expressions denote
&gt; &gt;&gt; the same things in all interpretations -- but the
&gt; &gt;&gt; more general construction eludes me.
&gt; 
&gt; In Ch 5, I defined the notion of a "meaning-preserving
&gt; translation", either within a single language or among
&gt; multiple languages.  The simplest example is identity,
&gt; but I define a more interesting class of transformations:
&gt; 
&gt;    <A  HREF="http://www.bestweb.net/~sowa/logic/meaning.htm">http://www.bestweb.net/~sowa/logic/meaning.htm</A>
&gt; 
&gt; That file has the nicely formatted version.
&gt; The plain text version is attached below.
&gt; 
&gt; JA&gt; This is exactly the sort of problem that
&gt; &gt;   Category Theory was invented to solve:
&gt; 
&gt; I agree, and I was thinking of category theory, but I didn't
&gt; want to overburden the KR book with too much formalism that I
&gt; didn't intend to apply in detail.  However, category theory
&gt; is so general that the really interesting questions are often
&gt; in the details of what kinds of transformations are appropriate
&gt; for any given application.
&gt; 
&gt; With that caveat, I agree that there are a lot of interesting
&gt; possibilities for using category theory.  Joe Goguen &amp; Co. have
&gt; been using "institution theory" for related purposes:
&gt; 
&gt;    <A  HREF="http://citeseer.nj.nec.com/384650.html">http://citeseer.nj.nec.com/384650.html</A>
&gt; 
&gt;   "Abstract: Institutions formalize the intuitive notion
&gt;   of logical system, including both syntax and semantics."
&gt; 
&gt; Following is the plain text version of the HTML file cited above.
&gt; 
&gt; John Sowa
&gt; 
&gt; ---------------------------------------------------------------
&gt; 
&gt;                    Meaning-Preserving Translations
&gt; 
&gt;                            by John F. Sowa
&gt; 
&gt; Informally, different statements in different languages can mean "the same
&gt; thing." Formally, that "thing," called a proposition, represents abstract,
&gt; language-independent, semantic content. As an abstraction, a proposition
&gt; has no physical embodiment that can be written or spoken. Only its
&gt; statements in particular languages can be expressed as strings of symbols.
&gt; 
&gt; According to Peirce (1905), "The meaning of a proposition is itself a
&gt; proposition. Indeed, it is no other than the very proposition of which it
&gt; is the meaning: it is a translation of it." Mathematically, Peirce's
&gt; informal statement may be formalized by defining a proposition as an
&gt; equivalence class of sentences that can be translated from one to another
&gt; while preserving meaning. Some further criteria are necessary to specify
&gt; what kinds of translations are considered to "preserve meaning." Formally,
&gt; a meaning-preserving translation f from a language L1 to a language L2 may
&gt; be defined as a function that satisfies the following constraints:
&gt; 
&gt;    * Invertible. The translation function f must have an inverse function g
&gt;      that maps sentences from L2 back to L1. For any sentence s in L1, f(s)
&gt;      is a sentence in L2, and g(f(s)) is a sentence in L1. All three
&gt;      sentences, s, f(s), and g(f(s)) are said to express the proposition p.
&gt; 
&gt;    * Proof preserving. When a sentence s in L1 is translated to f(s) in
&gt;      L2 and back again to g(f(s)) in L1, the result might not be identical
&gt;      to s. But according to the rules of inference of language L1, each
&gt;      one must be provable from the other: s|-g(f(s)), and g(f(s))|-s.
&gt;      Similarly, f(s) and f(g(f(s))) must be provable from each other by
&gt;      the rules of inference of language L2.
&gt; 
&gt;    * Vocabulary preserving. When s is translated from L1 to L2 and back to
&gt;      g(f(s)), the logical symbols like &amp;forall; and the syntactic markers
&gt;      like commas and parentheses might be replaced by some equivalent.
&gt;      However, the same content words or symbols that represent categories,
&gt;      relations, and individuals in the ontology must appear in both
&gt;      sentences s and g(f(s)). This criterion could be relaxed to allow
&gt;      terms to be replaced by synonyms or definitions, but arbitrary content
&gt;      words or predicates must not be added or deleted by the translations.
&gt; 
&gt;    * Structure preserving. When s and g(f(s)) are mapped to Peirce Normal
&gt;      Form (with negation ~, conjunction &amp;, and the existential quantifier
&gt;      &amp;exist; as the only logical operators), they must contain exactly
&gt;      the same number of negations and existential quantifiers, nested in
&gt;      semantically equivalent patterns.
&gt; 
&gt; These four criteria ensure that the sentences s and g(f(s)) are highly
&gt; similar, if not identical. If s is the sentence "Every farmer who owns a
&gt; donkey beats it", then the sentence g(f(s)) might be "If a farmer x owns
&gt; a donkey y, then x beats y". Those sentences use different logical and
&gt; syntactical symbols, but they are provably equivalent, they have the same
&gt; content words, and they have the same structure when expressed with only
&gt; &amp;, ~, and &amp;exist;.
&gt; 
&gt; Attempts to apply formal definitions to natural languages are fraught
&gt; with pitfalls, exceptions, and controversies. To avoid such problems, the
&gt; definition of meaning-preserving translation may be restricted to formal
&gt; languages, like CGs and KIF. The sample sentence in Figure 5.12 could be
&gt; defined as part of a formal language, controlled English, which happens
&gt; to contain many sentences that look like English. Yet even for formal
&gt; languages, the four criteria require further explanation and justification:
&gt; 
&gt;    * Invertible. The functions f and g are not exact inverses, since
&gt;      g(f(s)) might not be identical to s. To ensure that f is defined for
&gt;      all sentences in L1, the language L2 must be at least as expressive
&gt;      as L1. If L2 is more expressive than L1, then the inverse g might be
&gt;      undefined for some sentences in L2. In that case, the language L2
&gt;      would express a superset of the propositions of L1.
&gt; 
&gt;    * Proof preserving. Preserving provability is necessary for meaning
&gt;      preservation, but it is not sufficient. It is a weak condition that
&gt;      allows all tautologies to be considered equivalent, even though the
&gt;      proof of equivalence might take an exponential amount of time.
&gt;      Informally, the test to determine whether two sentences "mean the
&gt;      same" should be "obvious." Formally, it should be computable by an
&gt;      efficient algorithm -- one whose time is linearly or polynomially
&gt;      proportional to the length of the sentence.
&gt; 
&gt;    * Vocabulary preserving. Two sentences that mean the same should talk
&gt;      about the same things. The sentence Every cat is a cat is provably
&gt;      equivalent to Every dog is a dog, even though one is about cats and
&gt;      the other is about dogs. Even worse, both of them are provably
&gt;      equivalent to a sentence about nonexistent things, such as Every
&gt;      unicorn is a unicorn. An admissible translation could make some
&gt;      changes to the syntactic or logical symbols, as in the sentence If
&gt;      something is a cat, then it is a cat. It might replace the word cat
&gt;      with domestic feline, but it should not replace the word cat with dog
&gt;      or unicorn.
&gt; 
&gt;    * Structure preserving. Of all the logical operators, conjunction &amp;
&gt;      is the simplest and least controversial, while negation ~ introduces
&gt;      serious logical and philosophical problems. Intuitionists, for
&gt;      example, deny that ~~p is identical to p. For relevance logic,
&gt;      Anderson and Belnap (1975) disallowed the disjunctive syllogism, which
&gt;      is based on &amp;or; and ~, because it can introduce extraneous
&gt;      information into a proof. Computationally, ~~p and p have different
&gt;      effects on the binding of values to variables in Prolog, SQL, and many
&gt;      expert systems. The constraints on quantifiers and negations help
&gt;      ensure that formulas in the same equivalence class have the same
&gt;      properties of decidability and computational complexity.
&gt; 
&gt; These conditions impose strong constraints on translations that are said to
&gt; preserve meaning. They ensure that the content words or predicates remain
&gt; identical or synonymous, they preserve the logical structure, and they
&gt; prevent irrelevant content from being inserted.
&gt; 
&gt; Examples of Meaning-preserving Translations.  To illustrate the issues,
&gt; consider meaning-preserving translations between two different notations
&gt; for first-order logic. Let L1 be predicate calculus with Peano's symbols
&gt; &amp;, &amp;or;, ~, -&gt;, &amp;exist;, and &amp;forall;, and let L2 be predicate
&gt; calculus with Peirce's symbols +, x, -, -&lt;, &amp;Sigma;, and &amp;Pi;.
&gt; Then for any formulas or subformulas p and q in L1,
&gt; let f produce the following translations in L2:
&gt; 
&gt;    * Conjunction. p&amp;q =&gt; pxq.
&gt; 
&gt;    * Disjunction. p&amp;or;q =&gt; -(-px-q).
&gt; 
&gt;    * Negation. ~p =&gt; -p.
&gt; 
&gt;    * Implication. p-&gt;q =&gt; -(px-q).
&gt; 
&gt;    * Existential quantifier. (&amp;exist;x)p =&gt; &amp;Sigma;x p.
&gt; 
&gt;    * Universal quantifier. (&amp;forall;x)p =&gt; -&amp;Sigma;x -p.
&gt; 
&gt; The sentences generated by f use only the operators x, -, and
&gt; &amp;Sigma;, but the inverse g is defined for all operators in L2:
&gt; 
&gt;    * Conjunction. pxq =&gt; p&amp;q.
&gt; 
&gt;    * Disjunction. p+q =&gt; p&amp;or;q.
&gt; 
&gt;    * Negation. -p =&gt; ~p.
&gt; 
&gt;    * Implication. p-&lt;q =&gt; p-&gt;q.
&gt; 
&gt;    * Existential quantifier. &amp;Sigma;x p =&gt; (&amp;exist;x)p.
&gt; 
&gt;    * Universal quantifier. &amp;Pi;x p =&gt; (&amp;forall;x)p.
&gt; 
&gt; The functions f and g meet the criteria for meaning-preserving
&gt; translations: they are invertible, proof preserving, vocabulary preserving,
&gt; and structure preserving. Furthermore, the proof of equivalence can be
&gt; done in linear time by showing that two sentences s and t in L1 map to
&gt; the same form with the symbols &amp;, ~, and &amp;exist;.
&gt; 
&gt; The functions f and g in the previous example show that it is possible to
&gt; find functions that meet the four criteria. They don't map any sentences
&gt; to the same equivalence class unless they can be said to "preserve
&gt; meaning" in a very strict sense, but they leave many closely related
&gt; sentences in different classes: permutations such as p&amp;q and q&amp;p;
&gt; duplications such as p, p&amp;p, and p&amp;p&amp;p; and formulas with renamed
&gt; variables such as (&amp;exist;x)P(x) and (&amp;exist;y)P(y). To include more
&gt; such sentences in the same equivalence classes, a series of functions
&gt; f1, f2, ..., can be defined, all of which have the same inverse g:
&gt; 
&gt;   1. Sorting. The function f1 makes the same symbol replacements as f,
&gt;      but it also sorts conjunctions in alphabetical order. As a result,
&gt;      p&amp;q and q&amp;p in L1 would both be mapped to pxq in L2, which would be
&gt;      mapped by g back to p&amp;q. Therefore, f1 groups permutations in the
&gt;      same equivalence class. Since a list of N terms can be sorted in
&gt;      time proportional to NlogN, the function f1 takes just slightly
&gt;      longer than linear time.
&gt; 
&gt;   2. Renaming variables. The function f2 is like f1, but it also renames
&gt;      the variables to a standard sequence, such as x1 ,x2 , ... . For
&gt;      very long sentences with dozens of variables of the same type, the
&gt;      complexity of f2 could increase exponentially. A typed logic can
&gt;      help reduce the number of options, since the new variable names
&gt;      could be assigned in the same alphabetical order as their type
&gt;      labels. For the kinds of sentences used in human communications,
&gt;      most variables have different types, and the computation time for
&gt;      f2 would be nearly linear.
&gt; 
&gt;   3. Deleting duplicates. After f1 and f2 sort conjunctions and rename
&gt;      variables, the function f3 would eliminate duplicates by deleting
&gt;      any conjunct that is identical to the previous one. The deletions
&gt;      could be performed in linear time.
&gt; 
&gt; For the kinds of sentences that people speak and understand,
&gt; the total computation time of all three functions would be
&gt; nearly linear.  Although it is possible to construct sentences
&gt; whose computation time would increase exponentially, those
&gt; sentences would be hopelessly unintelligible to humans.
&gt; What is unnatural for humans would be inefficient for
&gt; computers.
&gt; 
&gt; This series of functions shows how large numbers of closely related
&gt; sentences can be reduced to a single canonical form. If two sentences
&gt; express the same proposition, their canonical forms, which can usually
&gt; be calculated efficiently, would be the same. The function f2 has the
&gt; effect of reducing sentences to Peirce Normal Form (PNF) -- the result
&gt; of translating a sentence from predicate calculus to an existential graph
&gt; and back again. As an example, consider the following sentence, which
&gt; Leibniz called the Praeclarum Theorema (splendid theorem):
&gt; 
&gt;    * ((p -&gt; r) &amp; (q -&gt; s)) -&gt; ((p &amp; q) -&gt; (r &amp; s)).
&gt; 
&gt; This formula may be read "If p implies r and q implies s, then p and q
&gt; imply r and s." When translated to L2 by f3 and back to L1 by g, it has
&gt; the following Peirce Normal Form:
&gt; 
&gt;    * ~((~(p &amp; ~r) &amp; ~(q &amp; ~s)) &amp; ~(~(p &amp; q) &amp; ~(r &amp; s)) ).
&gt; 
&gt; This form is not as readable as the original, but it serves as the
&gt; canonical representative of an equivalence class that contains 864
&gt; different, but highly similar sentences. The function f3, which deletes
&gt; duplicates, can reduce an infinite number of sentences to the same form.
&gt; Such transformations can factor out the differences caused by the choice
&gt; of symbols or syntax.
&gt; 
&gt; To account for synonyms and definitions, another function f4 could be used
&gt; to replace terms by their defining lambda expressions. If recursions are
&gt; allowed, the replacements and expansions would be equivalent in computing
&gt; power to a Turing machine; they could take exponential amounts of time or
&gt; even be undecidable.  Therefore, f4 should only expand definitions without
&gt; recursions, direct or indirect.  Since the definitions may introduce
&gt; permutations, duplications, and renamed variables, f4 should expand
&gt; the definitions before performing the reductions computed by f3.
&gt; Without recursion, the expansions would take at most polynomial time.
&gt; 
&gt; Meaning in Natural Languages.  When functions like the fi series are
&gt; extended to natural languages, they become deeply involved with the
&gt; problems of syntax, semantics, and pragmatics. In his early work on
&gt; transformational grammar, Noam Chomsky (1957) hoped to define
&gt; transformations as meaning-preserving functions. But the transformations
&gt; that moved phrases and subphrases had the effect of changing the scope of
&gt; quantifiers and the binding of pronouns to their antecedents:
&gt; 
&gt;    * Every cat chased some mouse.
&gt;      =&gt; Some mouse was chased by every cat.
&gt; 
&gt;    * We do your laundry by hand; we don't tear it by machine.
&gt;      =&gt; We don't tear your laundry by machine; we do it by hand.
&gt; 
&gt; To account for the implications of such transformations,
&gt; Chomsky (1982) developed his theory of government and binding,
&gt; which replaced all transformations by a single operator called
&gt; move-&amp;alpha; and a set of constraints on where the phrase &amp;alpha;
&gt; could be moved. In his most recent minimalist theory, Chomsky (1995)
&gt; eliminated movement altogether and formulated the principles of grammar
&gt; as a set of logical constraints.  With that theory, both language generation
&gt; and interpretation become constraint-satisfaction problems of the kind discussed
&gt; in Section 4.6.  The common thread running through these theories is Chomsky's search
&gt; for a syntax-based characterization of the meaning-preserving translations.
&gt; 
&gt; AI-based computational linguistics has also involved a search for
&gt; meaning-preserving translations, but with more emphasis on semantics
&gt; and pragmatics than on syntax.  Roger Schank (1975), for example, developed
&gt; his conceptual dependency theory as a canonical representation of meaning
&gt; with an ontology of eleven primitive action types.  Although Schank was
&gt; strongly opposed to formalization of any kind, his method of reducing
&gt; a sentence to canonical form could be viewed as a version of function f4.
&gt; In his later work (Schank &amp; Abelson 1977;  Schank 1982), he went beyond
&gt; the sentence to higher-level structures called scripts, memory organization
&gt; packets (MOPs), and thematic organization packets (TOPs).  These structures,
&gt; which have been implemented in framelike and graphlike versions of EC logic,
&gt; address meaning at the level of paragraphs and stories.  Stuart Shapiro and
&gt; his colleagues have implemented versions of propositional semantic networks,
&gt; which support similar structures in a form that maps more directly to logic
&gt; (Shapiro 1979; Shapiro &amp; Maida 1982; Shapiro &amp; Rappaport 1992).  Shapiro's
&gt; propositional nodes serve the same purpose as Peirce's ovals and McCarthy's
&gt; contexts.
&gt; 
&gt; Besides the structural forms of syntax and logic, the meaning-preserving
&gt; translations for natural languages must account for the subtle interactions
&gt; of many thousands of words. The next two sentences, for example, were
&gt; adapted from a news report on finance:
&gt; 
&gt;    * The latest economic indicators eased concerns that inflation is
&gt;      increasing.
&gt; 
&gt;    * The latest economic indicators heightened concerns that inflation is
&gt;      increasing.
&gt; 
&gt; The first sentence implies that inflation is not increasing, but the second
&gt; one implies that it is.  The negation, which is critical for understanding
&gt; the sentences, does not appear explicitly.  Instead, it comes from an
&gt; implicit negation in the meaning of the noun concern: if some agent x has
&gt; a concern about y, then x hopes that some bad event does not happen to y.
&gt; The concern is eased when the bad event is less likely to occur, and the
&gt; concern is heightened when the bad event is more likely to occur.  In the
&gt; normal use of language, people understand such sentences and their implications.
&gt; For a computer to understand them, it would require detailed definitions of the
&gt; words, background knowledge that rising inflation is bad for the economy, and
&gt; the reasoning ability to combine such information.  Doug Lenat and his group
&gt; in the Cyc project have been working since 1984 on the task of encoding and
&gt; reasoning with the millions of rules and facts needed for such understanding.

¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤

John,

This very interesting to me, but I am trying
to work in recursed spite of a headache today,
and so I am looking forwared to returning to ut
in detail when the pain stops.  I have of course
been specializing on the purely alpha level aspects
of this, and so it is probably best if I limit my
observations to what I know.  Just off the top
of my head -- to coin a poignant phrase --
think about the sixteen "propositions",
that is, abstract truth functions on
two boolean vraiables, in the lattice
of the implication ordering.  This is
the object domain.  Then think about
all the various and sundry notational
schemes that you have seen for signing
these elements of this object structure.
The "logical equivalence classes" (LEC's)
of "sentences" (propositional expressions)
in each language then "reconstruct" over the
syntactic domain the structure of the object
domain.  What sorts of factors would enter into
your chosing one language over another for this?
The degree of clutter in the LEC's, the difficulty
of getting to the canonical representatives of LEC's,
and so on, are factors that weigh against a particular
language.  In this regard, Peirce's Alpha is pretty good,
while most of the languages that we still teach students are
a positive impediment to logical representation and reasoning.

Nuf Fer Now,

Jon Awbrey

¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤~~~~~~~~~¤

</PRE>

<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
<hr>
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="01850" href="msg01850.html">SUO: Re: Propositions</a></strong>
<ul><li><em>From:</em> Jon Awbrey &lt;jawbrey@oakland.edu&gt;</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="01822" href="msg01822.html">SUO: Propositions (was Peirce's MS 514)</a></strong>
<ul><li><em>From:</em> &quot;John F. Sowa&quot; &lt;sowa@bestweb.net&gt;</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
<ul>
<LI>Prev by Date:
<STRONG><A HREF="msg01837.html">SUO: Re: 'q'</A></STRONG>
</LI>
<LI>Next by Date:
<STRONG><A HREF="msg01835.html">Re: SUO: RE: KIF syntax and semantics and a basic ontology</A></STRONG>
</LI>
<li>Prev by thread:
<strong><a href="msg01825.html">Re: SUO: Propositions (was Peirce's MS 514)</a></strong>
</li>
<li>Next by thread:
<strong><a href="msg01850.html">SUO: Re: Propositions</a></strong>
</li>
<li>Index(es):
<ul>
<li><a href="mail54.html#01836"><strong>Date</strong></a></li>
<li><a href="thrd53.html#01836"><strong>Thread</strong></a></li>
</ul>
</li>
</ul>

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<!--X-User-Footer-End-->
</body>
</html>
